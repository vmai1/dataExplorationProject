{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------\n",
    "#\n",
    "#  KICKSTARTER.COM WEB SCRAPER\n",
    "#\n",
    "#-----------------------------------------------------------------------\n",
    "#  \n",
    "#  Written for Python 3.3\n",
    "#\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "import urllib3\n",
    "import bs4\n",
    "import csv\n",
    "import re\n",
    "import datetime\n",
    "# import os\n",
    "\n",
    "#======================\n",
    "# Initialized variables\n",
    "#======================\n",
    "\n",
    "search = 'cards'\n",
    "\n",
    "BASE_URL = 'https://www.kickstarter.com'\n",
    "first_url = 'https://www.kickstarter.com/projects/search?page=1&term=' + search\n",
    "subcat = 'https://www.kickstarter.com/discover/categories/34'\n",
    "headers = ['URL','Title','Category','Start Date', 'End Date', 'Goal','Funding Amount','Backers','# of Rewards','Reward Price','# Claimed']\n",
    "\n",
    "search_results = [first_url]\n",
    "to_crawl = []\n",
    "crawled = []\n",
    "\n",
    "#======================\n",
    "# addUrl\n",
    "# Check if URL is in either list\n",
    "# Add an individual URL to specified list\n",
    "#======================\n",
    "def addUrl(url,in_list):\n",
    "    if url not in search_results and url not in to_crawl and url not in crawled:\n",
    "        in_list.append(url)\n",
    "\n",
    "#======================\n",
    "# soupify\n",
    "# Turns an html page into soup for further manipulation\n",
    "#======================\n",
    "def soupify(url):\n",
    "    doc = urllib3.PoolManager().request('GET',url)\n",
    "    return bs4.BeautifulSoup(doc.data)\n",
    "\n",
    "#======================\n",
    "# numify\n",
    "# Remove letters and characters from string and turn into integer\n",
    "#======================\n",
    "def numify(in_str):\n",
    "    newNum = int(re.sub('[^0-9]', '', in_str))\n",
    "    return newNum\n",
    "\n",
    "#======================\n",
    "# addSearchPages\n",
    "# Run this once first at the beginning of search\n",
    "# Start with first URL and add all the other pages from the search result\n",
    "#======================\n",
    "def addSearchPages(url):\n",
    "    '''\n",
    "    Takes in a URL. \n",
    "    Generates list of pages that needs to be crawled based on last page number.\n",
    "    Returns nothing\n",
    "    '''\n",
    "    soup = soupify(url)\n",
    "    lastPage = soup.find('div',{'class':'pagination'}).find_all('a')[-2].string\n",
    "    for i in range(2,int(lastPage) + 1):\n",
    "        newUrl = BASE_URL + \"/projects/search?page=\" + str(i) + \"&term=\" + search\n",
    "        addUrl(newUrl, search_results)\n",
    "\n",
    "#======================\n",
    "# addProjectPages\n",
    "# Check if URL is in either list\n",
    "# Add an individual URL\n",
    "#======================\n",
    "def addProjectPages(url):\n",
    "    '''\n",
    "    Takes in one URL from search_results from the list\n",
    "    Appends project page URLs to to_crawl\n",
    "    Returns nothing\n",
    "    '''\n",
    "    soup = soupify(url)\n",
    "    for projects in soup.find_all('h2',{'class':'bbcard_name'}):\n",
    "        for link in projects.find_all('a'):\n",
    "            newUrl = BASE_URL + link.get('href')[0:-11]\n",
    "            addUrl(newUrl, to_crawl)\n",
    "\n",
    "#======================\n",
    "# extractData\n",
    "# Identify key parts of each project\n",
    "# Title, category, start date, end date, goal, funded or not, # of rewards, reward level, # of backers\n",
    "#======================\n",
    "def extractData(url):\n",
    "    '''\n",
    "    Takes in URL\n",
    "    Returns title, category, goal, funding status, total backers, # or rewards, reward level, # of backers\n",
    "    '''\n",
    "    soup = soupify(url)\n",
    "    title = soup.head.title.string[0:-14]\n",
    "    title = str(re.sub(r'[^\\x00-\\x7f]', '', title))\n",
    "    category = str(soup.find('li',{'class':'category'}).find('a').contents[1][1:-1])\n",
    "    start = soup.find_all('time')[-2].string\n",
    "    end = soup.find_all('time')[-1].string\n",
    "    goal = float(soup.find(id='pledged')['data-goal'])\n",
    "    funded = float(soup.find(id='pledged').find('data')['data-value'])\n",
    "    backers = numify(soup.find(id='backers_count').find('data').contents[0])\n",
    "    rewards = soup.find(id='what-you-get').find_all('li')\n",
    "    reward_price = []\n",
    "    reward_back = []\n",
    "    for reward in rewards:\n",
    "        reward_price.append(numify(reward.h5.span.string))\n",
    "        reward_back.append(int(reward.find('span',{'class':'num-backers'}).string[1:].split(' ')[0]))\n",
    "    return title, category, start, end, goal, funded, backers, len(rewards), reward_price, reward_back\n",
    "\n",
    "#======================\n",
    "# writeCSV\n",
    "# Create new CSV file including\n",
    "# Write URL, and all info from extractData(url)\n",
    "#======================\n",
    "def writeCSV(crawl_list,time,errors):\n",
    "    '''\n",
    "    Takes in a list of URLs to crawl and the time to create unique filename\n",
    "    Writes important data to CSV\n",
    "    Returns nothing\n",
    "    '''\n",
    "    newT = str(numify(str(time)))\n",
    "    newF = open(search+'_'+newT+'.csv', 'w', newline='')\n",
    "    writer = csv.writer(newF, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(headers)\n",
    "    while len(crawl_list) > 0:\n",
    "        project = crawl_list.pop()\n",
    "        print(project)\n",
    "        try:\n",
    "            t, c, s,e, g, f, b, r, rp, rb = extractData(project)\n",
    "            row = [project, t, c, s, e, g, f, b, r]\n",
    "            for i in range(len(rp)):\n",
    "                row.append(rp[i])\n",
    "                row.append(rb[i])\n",
    "            writer.writerow(row)\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            row = [project, e]\n",
    "            writer.writerow(row)\n",
    "            print(e)\n",
    "        crawled.append(project)\n",
    "    newF.close()\n",
    "    return\n",
    "\n",
    "#======================\n",
    "# startCrawl\n",
    "# Start the crawl process\n",
    "#======================\n",
    "def startCrawl():\n",
    "    errors = 0\n",
    "    startTime = datetime.datetime.now()\n",
    "\n",
    "    addSearchPages(search_results[0])\n",
    "\n",
    "    while len(search_results) > 0:\n",
    "        current = search_results.pop()\n",
    "        print(current)\n",
    "        try:\n",
    "            addProjectPages(current)\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            print(e)\n",
    "            continue\n",
    "        crawled.append(current)\n",
    "    projects = len(to_crawl)\n",
    "\n",
    "    writeCSV(to_crawl,startTime,errors)\n",
    "\n",
    "    # try:\n",
    "    #     writeCSV(to_crawl,startTime)\n",
    "    # except Exception as e:\n",
    "    #     errors += 1\n",
    "    #     print(e)\n",
    "\n",
    "    print('Number of crawled pages: ' + str(len(crawled)))\n",
    "    print('Number of projects: ' + str(projects))\n",
    "    print('Number of errors: ' + str(errors))\n",
    "    print('Elapsed time: ' + str(datetime.datetime.now() - startTime))\n",
    "\n",
    "startCrawl()\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "#======================\n",
    "# Testing\n",
    "#======================\n",
    "# to_crawl = ['https://www.kickstarter.com/projects/1761852825/the-aesir-deck-viking-gods-on-playing-cards',\n",
    "# 'https://www.kickstarter.com/projects/1374838500/japanese-the-game-a-language-learning-card-game']\n",
    "\n",
    "# print(extractData(to_crawl[0]))\n",
    "# print(extractData(to_crawl[1]))\n",
    "\n",
    "# extractData(to_crawl[0])\n",
    "# extractData(to_crawl[1])\n",
    "\n",
    "# time = datetime.datetime.now()\n",
    "# writeCSV(to_crawl,time,errors=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
