{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  KICKSTARTER.COM WEB SCRAPER\n",
    "  \n",
    "##  Written for Python 3.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import bs4\n",
    "import csv\n",
    "import re\n",
    "import datetime\n",
    "# import os\n",
    "\n",
    "#======================\n",
    "# Initialized variables\n",
    "#======================\n",
    "\n",
    "search = 'cards'\n",
    "\n",
    "BASE_URL = 'https://www.kickstarter.com'\n",
    "first_url = 'https://www.kickstarter.com/projects/search?page=1&term=' + search\n",
    "subcat = 'https://www.kickstarter.com/discover/categories/34'\n",
    "headers = ['URL','Title','Category','Start Date', 'End Date', 'Goal','Funding Amount','Backers','# of Rewards','Reward Price','# Claimed']\n",
    "\n",
    "search_results = [first_url]\n",
    "to_crawl = []\n",
    "crawled = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================\n",
    "# addUrl\n",
    "# Check if URL is in either list\n",
    "# Add an individual URL to specified list\n",
    "#======================\n",
    "def addUrl(url,in_list):\n",
    "    if url not in search_results and url not in to_crawl and url not in crawled:\n",
    "        in_list.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================\n",
    "# soupify\n",
    "# Turns an html page into soup for further manipulation\n",
    "#======================\n",
    "def soupify(url):\n",
    "    doc = urllib3.PoolManager().request('GET',url)\n",
    "    return bs4.BeautifulSoup(doc.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================\n",
    "# numify\n",
    "# Remove letters and characters from string and turn into integer\n",
    "#======================\n",
    "def numify(in_str):\n",
    "    newNum = int(re.sub('[^0-9]', '', in_str))\n",
    "    return newNum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================\n",
    "# addSearchPages\n",
    "# Run this once first at the beginning of search\n",
    "# Start with first URL and add all the other pages from the search result\n",
    "#======================\n",
    "def addSearchPages(url):\n",
    "    '''\n",
    "    Takes in a URL. \n",
    "    Generates list of pages that needs to be crawled based on last page number.\n",
    "    Returns nothing\n",
    "    '''\n",
    "    soup = soupify(url)\n",
    "    lastPage = soup.find('div',{'class':'pagination'}).find_all('a')[-2].string\n",
    "    for i in range(2,int(lastPage) + 1):\n",
    "        newUrl = BASE_URL + \"/projects/search?page=\" + str(i) + \"&term=\" + search\n",
    "        addUrl(newUrl, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================\n",
    "# addProjectPages\n",
    "# Check if URL is in either list\n",
    "# Add an individual URL\n",
    "#======================\n",
    "def addProjectPages(url):\n",
    "    '''\n",
    "    Takes in one URL from search_results from the list\n",
    "    Appends project page URLs to to_crawl\n",
    "    Returns nothing\n",
    "    '''\n",
    "    soup = soupify(url)\n",
    "    for projects in soup.find_all('h2',{'class':'bbcard_name'}):\n",
    "        for link in projects.find_all('a'):\n",
    "            newUrl = BASE_URL + link.get('href')[0:-11]\n",
    "            addUrl(newUrl, to_crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================\n",
    "# extractData\n",
    "# Identify key parts of each project\n",
    "# Title, category, start date, end date, goal, funded or not, # of rewards, reward level, # of backers\n",
    "#======================\n",
    "def extractData(url):\n",
    "    '''\n",
    "    Takes in URL\n",
    "    Returns title, category, goal, funding status, total backers, # or rewards, reward level, # of backers\n",
    "    '''\n",
    "    soup = soupify(url)\n",
    "    title = soup.head.title.string[0:-14]\n",
    "    title = str(re.sub(r'[^\\x00-\\x7f]', '', title))\n",
    "    category = str(soup.find('li',{'class':'category'}).find('a').contents[1][1:-1])\n",
    "    start = soup.find_all('time')[-2].string\n",
    "    end = soup.find_all('time')[-1].string\n",
    "    goal = float(soup.find(id='pledged')['data-goal'])\n",
    "    funded = float(soup.find(id='pledged').find('data')['data-value'])\n",
    "    backers = numify(soup.find(id='backers_count').find('data').contents[0])\n",
    "    rewards = soup.find(id='what-you-get').find_all('li')\n",
    "    reward_price = []\n",
    "    reward_back = []\n",
    "    for reward in rewards:\n",
    "        reward_price.append(numify(reward.h5.span.string))\n",
    "        reward_back.append(int(reward.find('span',{'class':'num-backers'}).string[1:].split(' ')[0]))\n",
    "    return title, category, start, end, goal, funded, backers, len(rewards), reward_price, reward_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================\n",
    "# writeCSV\n",
    "# Create new CSV file including\n",
    "# Write URL, and all info from extractData(url)\n",
    "#======================\n",
    "def writeCSV(crawl_list,time,errors):\n",
    "    '''\n",
    "    Takes in a list of URLs to crawl and the time to create unique filename\n",
    "    Writes important data to CSV\n",
    "    Returns nothing\n",
    "    '''\n",
    "    newT = str(numify(str(time)))\n",
    "    newF = open(search+'_'+newT+'.csv', 'w', newline='')\n",
    "    writer = csv.writer(newF, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(headers)\n",
    "    while len(crawl_list) > 0:\n",
    "        project = crawl_list.pop()\n",
    "        print(project)\n",
    "        try:\n",
    "            t, c, s,e, g, f, b, r, rp, rb = extractData(project)\n",
    "            row = [project, t, c, s, e, g, f, b, r]\n",
    "            for i in range(len(rp)):\n",
    "                row.append(rp[i])\n",
    "                row.append(rb[i])\n",
    "            writer.writerow(row)\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            row = [project, e]\n",
    "            writer.writerow(row)\n",
    "            print(e)\n",
    "        crawled.append(project)\n",
    "    newF.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================\n",
    "# startCrawl\n",
    "# Start the crawl process\n",
    "#======================\n",
    "def startCrawl():\n",
    "    errors = 0\n",
    "    startTime = datetime.datetime.now()\n",
    "\n",
    "    addSearchPages(search_results[0])\n",
    "\n",
    "    while len(search_results) > 0:\n",
    "        current = search_results.pop()\n",
    "        print(current)\n",
    "        try:\n",
    "            addProjectPages(current)\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            print(e)\n",
    "            continue\n",
    "        crawled.append(current)\n",
    "    projects = len(to_crawl)\n",
    "\n",
    "    writeCSV(to_crawl,startTime,errors)\n",
    "\n",
    "    # try:\n",
    "    #     writeCSV(to_crawl,startTime)\n",
    "    # except Exception as e:\n",
    "    #     errors += 1\n",
    "    #     print(e)\n",
    "\n",
    "    print('Number of crawled pages: ' + str(len(crawled)))\n",
    "    print('Number of projects: ' + str(projects))\n",
    "    print('Number of errors: ' + str(errors))\n",
    "    print('Elapsed time: ' + str(datetime.datetime.now() - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maiv2\\AppData\\Local\\Continuum\\anaconda3\\envs\\condadatascience\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\maiv2\\AppData\\Local\\Continuum\\anaconda3\\envs\\condadatascience\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\maiv2\\AppData\\Local\\Continuum\\anaconda3\\envs\\condadatascience\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8b489a4dfb9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstartCrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-677283699f17>\u001b[0m in \u001b[0;36mstartCrawl\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mstartTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0maddSearchPages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_results\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-32dac481f0b5>\u001b[0m in \u001b[0;36maddSearchPages\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     11\u001b[0m     '''\n\u001b[0;32m     12\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoupify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mlastPage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'pagination'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlastPage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mnewUrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBASE_URL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/projects/search?page=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"&term=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "startCrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a5f3fe537251>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.kickstarter.com/discover/advanced?ref=discovery_overlay.json'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#r = requests.get('https://www.kickstarter.com/discover/advanced.json?category_id=0&woe_id=0&sort=most_funded&page=' + str(page))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"projects\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#print \"%s,%f,%s,%f\" % (data[\"projects\"][index][\"name\"], data[\"projects\"][index][\"goal\"], data[\"projects\"][index][\"currency\"], data[\"projects\"][index][\"pledged\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\condadatascience\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    890\u001b[0m                     \u001b[1;31m# used.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\condadatascience\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\condadatascience\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \"\"\"\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\condadatascience\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "#data outputs to a CSV file in the current directory\n",
    "csv_output = open(\"top-funded-sample.csv\", \"w\")\n",
    "\n",
    "end_page = 77;\n",
    "\n",
    "#scan through pages 1 to end_page for data, 20 results per page\n",
    "for page in range(1,end_page+1):\n",
    "    r = requests.get('https://www.kickstarter.com/discover/advanced?ref=discovery_overlay.json' + str(page))\n",
    "    #r = requests.get('https://www.kickstarter.com/discover/advanced.json?category_id=0&woe_id=0&sort=most_funded&page=' + str(page))\n",
    "    data = r.json()\n",
    "    for index in range(len(data[\"projects\"])):\n",
    "        #print \"%s,%f,%s,%f\" % (data[\"projects\"][index][\"name\"], data[\"projects\"][index][\"goal\"], data[\"projects\"][index][\"currency\"], data[\"projects\"][index][\"pledged\"])\n",
    "        csv_output.write(\"\\\"%s\\\",%s,%.0f,%s,%.2f,%d,%s,%s,%s\\n\" % (data[\"projects\"][index][\"name\"].encode('ascii', 'ignore'), \n",
    "            data[\"projects\"][index][\"category\"][\"slug\"].split(\"/\")[0],\n",
    "            data[\"projects\"][index][\"goal\"], \n",
    "            data[\"projects\"][index][\"currency\"], \n",
    "            data[\"projects\"][index][\"pledged\"],\n",
    "            data[\"projects\"][index][\"backers_count\"],\n",
    "            str(datetime.datetime.fromtimestamp(data[\"projects\"][index][\"created_at\"])),\n",
    "            str(datetime.datetime.fromtimestamp(data[\"projects\"][index][\"launched_at\"])),\n",
    "            str(datetime.datetime.fromtimestamp(data[\"projects\"][index][\"deadline\"]))))\n",
    "\n",
    "csv_output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
